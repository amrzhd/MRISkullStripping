{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrzhd/MRISkullStripping/blob/main/MRI_Skull_Stripping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN1J4XX6c-T6"
      },
      "source": [
        "#MRI Skull Stripping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JlrYL-Th72w"
      },
      "source": [
        "# Dataset Installation Using DataLad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oczu6Lj9sHxR",
        "outputId": "b0413e1f-d370-49b8-a354-5e8cdea3a9b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "datalad is already the newest version (0.15.5-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install datalad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGfhhOD_uBLo",
        "outputId": "9db115af-cf54-4d40-fe4d-ce5fb6c7ef7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n",
            "datalad 0.15.5\n"
          ]
        }
      ],
      "source": [
        "!datalad --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfkfvoM-w4nP",
        "outputId": "311c73e6-29e9-405c-822f-209e50f34ad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "git-annex version: 8.20210223\n",
            "build flags: Assistant Webapp Pairing Inotify DBus DesktopNotify TorrentParser MagicMime Feeds Testsuite S3 WebDAV\n",
            "dependency versions: aws-0.22 bloomfilter-2.0.1.0 cryptonite-0.26 DAV-1.3.4 feed-1.3.0.1 ghc-8.8.4 http-client-0.6.4.1 persistent-sqlite-2.10.6.2 torrent-10000.1.1 uuid-1.3.13 yesod-1.6.1.0\n",
            "key/value backends: SHA256E SHA256 SHA512E SHA512 SHA224E SHA224 SHA384E SHA384 SHA3_256E SHA3_256 SHA3_512E SHA3_512 SHA3_224E SHA3_224 SHA3_384E SHA3_384 SKEIN256E SKEIN256 SKEIN512E SKEIN512 BLAKE2B256E BLAKE2B256 BLAKE2B512E BLAKE2B512 BLAKE2B160E BLAKE2B160 BLAKE2B224E BLAKE2B224 BLAKE2B384E BLAKE2B384 BLAKE2BP512E BLAKE2BP512 BLAKE2S256E BLAKE2S256 BLAKE2S160E BLAKE2S160 BLAKE2S224E BLAKE2S224 BLAKE2SP256E BLAKE2SP256 BLAKE2SP224E BLAKE2SP224 SHA1E SHA1 MD5E MD5 WORM URL X*\n",
            "remote types: git gcrypt p2p S3 bup directory rsync web bittorrent webdav adb tahoe glacier ddar git-lfs httpalso borg hook external\n",
            "operating system: linux x86_64\n",
            "supported repository versions: 8\n",
            "upgrade supported from repository versions: 0 1 2 3 4 5 6 7\n"
          ]
        }
      ],
      "source": [
        "!git annex version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcCCPSHTiSpN",
        "outputId": "5a226446-0f63-4954-8780-0818c5172f88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n"
          ]
        }
      ],
      "source": [
        "!datalad install https://github.com/CONP-PCNO/conp-dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqbTerz_2ybE",
        "outputId": "f00c9254-a501-4f11-ccd1-6dbd3fc265e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n",
            "Clone attempt:   0% 0.00/4.00 [00:00<?, ? Candidate locations/s]\n",
            "Enumerating: 0.00 Objects [00:00, ? Objects/s]\u001b[A\n",
            "                                              \u001b[A\n",
            "Counting:   0% 0.00/243 [00:00<?, ? Objects/s]\u001b[A\n",
            "                                              \u001b[A\n",
            "Compressing: 0.00 Objects [00:00, ? Objects/s]\u001b[A\n",
            "\n",
            "Receiving:   0% 0.00/243 [00:00<?, ? Objects/s]\u001b[A\u001b[A\n",
            "\n",
            "                                               \u001b[A\u001b[A\n",
            "\n",
            "Resolving:   0% 0.00/60.0 [00:00<?, ? Deltas/s]\u001b[A\u001b[A\n",
            "\n",
            "                                               \u001b[A\u001b[A\n",
            "[INFO   ] Author identity unknown \n",
            "[INFO   ] *** Please tell me who you are.\n",
            "| \n",
            "| Run\n",
            "| \n",
            "|   git config --global user.email \"you@example.com\"\n",
            "|   git config --global user.name \"Your Name\"\n",
            "| \n",
            "| to set your account's default identity.\n",
            "| Omit --global to set the identity only in this repository. \n",
            "[INFO   ] fatal: unable to auto-detect email address (got 'root@a279a5ba8b46.(none)') \n",
            "[INFO   ] scanning for unlocked files (this may take some time) \n",
            "[INFO   ] Remote origin not usable by git-annex; setting annex-ignore \n",
            "\u001b[1;1minstall\u001b[0m(\u001b[1;32mok\u001b[0m): projects/calgary-campinas (\u001b[1;35mdataset\u001b[0m) [Installed subdataset in order to get /content/conp-dataset/projects/calgary-campinas]\n"
          ]
        }
      ],
      "source": [
        "!datalad install -d conp-dataset conp-dataset/projects/calgary-campinas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlRYhDg0--1u",
        "outputId": "7d84e128-f1ae-4e04-be94-4c4f1723486e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_part1.zip  Train_part2.zip  Train_part3.zip  Val.zip\n"
          ]
        }
      ],
      "source": [
        "!ls conp-dataset/projects/calgary-campinas/CC359/Raw-data/Single-channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npLqf4bFyTjl",
        "outputId": "4442bb49-8b34-4f6c-d190-d2cb8d403903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n",
            "Total:   0% 0.00/107G [00:00<?, ? Bytes/s]\n",
            "Get CC359/Re .. rrection.zip:   0% 0.00/2.31M [00:00<?, ? Bytes/s]\u001b[A\n",
            "Total:   0% 2.31M/107G [00:01<13:13:10, 2.25M Bytes/s]\n",
            "Get CC359/Re .. Original.zip:   0% 0.00/4.15G [00:00<?, ? Bytes/s]\u001b[A\n",
            "Get CC359/Re .. Original.zip:   0% 10.8M/4.15G [00:00<03:10, 21.7M Bytes/s]\u001b[A\n",
            "Get CC359/Re .. Original.zip:   0% 16.2M/4.15G [00:01<04:31, 15.2M Bytes/s]\u001b[A\n",
            "Get CC359/Re .. Original.zip:   1% 21.9M/4.15G [00:01<05:06, 13.5M Bytes/s]\u001b[A\n",
            "Get CC359/Re .. Original.zip:   1% 28.2M/4.15G [00:02<05:14, 13.1M Bytes/s]\u001b[A\n",
            "Get CC359/Re .. Original.zip:   1% 34.0M/4.15G [00:02<05:27, 12.6M Bytes/s]\u001b[A\n",
            "Get CC359/Re .. Original.zip:   1% 40.0M/4.15G [00:03<05:32, 12.4M Bytes/s]\u001b[A\n",
            "Get CC359/Re .. Original.zip:   1% 45.7M/4.15G [00:03<05:41, 12.0M Bytes/s]\u001b[ATraceback (most recent call last):\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/interface/base.py\", line 786, in call_from_parser\n",
            "    ret = list(ret)\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/interface/utils.py\", line 396, in generator_func\n",
            "    for r in _process_results(\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/interface/utils.py\", line 579, in _process_results\n",
            "    for res in results:\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/distribution/get.py\", line 967, in __call__\n",
            "    for res in _get_targetpaths(\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/distribution/get.py\", line 671, in _get_targetpaths\n",
            "    results = ds_repo.get(\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/support/gitrepo.py\", line 316, in _wrap_normalize_paths\n",
            "    result = func(self, files_new, *args, **kwargs)\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/support/annexrepo.py\", line 1450, in get\n",
            "    results = self._call_annex_records(\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/support/annexrepo.py\", line 1044, in _call_annex_records\n",
            "    out = self._call_annex(\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/support/annexrepo.py\", line 940, in _call_annex\n",
            "    return runner.run_on_filelist_chunks(\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/runner/gitrunner.py\", line 189, in run_on_filelist_chunks\n",
            "    res = self.run(\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/runner/runner.py\", line 126, in run\n",
            "    results = run_command(\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/runner/nonasyncrunner.py\", line 249, in run_command\n",
            "    file_number, data, time_stamp = output_queue.get()\n",
            "  File \"/usr/lib/python3.10/queue.py\", line 171, in get\n",
            "    self.not_empty.wait()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 320, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/bin/datalad\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('datalad==0.15.5', 'console_scripts', 'datalad')())\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/cmdline/main.py\", line 216, in main\n",
            "    ret = cmdlineargs.func(cmdlineargs)\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/interface/base.py\", line 790, in call_from_parser\n",
            "    % CapturedException(exc))\n",
            "  File \"/usr/lib/python3/dist-packages/datalad/support/exceptions.py\", line 51, in __init__\n",
            "    self.tb = traceback.TracebackException.from_exception(\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 572, in from_exception\n",
            "    return cls(type(exc), exc, exc.__traceback__, *args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 383, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 306, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 137, in updatecache\n",
            "    lines = fp.readlines()\n",
            "  File \"/usr/lib/python3.10/codecs.py\", line 319, in decode\n",
            "    def decode(self, input, final=False):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!datalad get -d conp-dataset/projects/calgary-campinas conp-dataset/projects/calgary-campinas/CC359/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoOkszKBB7nv"
      },
      "source": [
        "#Installing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxXm35cmEpAE",
        "outputId": "889d5fef-ce9f-4193-91c5-7bbd0e028660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn\n",
            "  Downloading nilearn-0.10.4-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.4.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (5.3.0)\n",
            "Requirement already satisfied: nibabel>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (24.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.13.1)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel>=4.0.0->nilearn) (6.4.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from nibabel>=4.0.0->nilearn) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->nilearn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->nilearn) (1.16.0)\n",
            "Downloading nilearn-0.10.4-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nilearn\n",
            "Successfully installed nilearn-0.10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woCNnWJTEsMu",
        "outputId": "a1b4c120-e35a-487f-e32c-2afd4e560721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (5.3.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel) (6.4.5)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from nibabel) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from nibabel) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from nibabel) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nibabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWnT95HhifZk",
        "outputId": "1984caca-d4a8-481e-b1f9-10b6470efce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monai\n",
            "  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.10/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (3.0.2)\n",
            "Downloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: monai\n",
            "Successfully installed monai-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install monai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_SDXx70XSgG",
        "outputId": "107546ba-9fb3-443c-d14b-67a29dd36d10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im4aucsWdm-E"
      },
      "source": [
        "#Libraries Used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YXO5dMRWdsN2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Scikit Learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# NI\n",
        "import nibabel as nib\n",
        "from nilearn.image import smooth_img, resample_to_img\n",
        "from nilearn.masking import apply_mask\n",
        "\n",
        "# Monai\n",
        "import monai\n",
        "from monai.transforms import (Compose, LoadImaged, EnsureChannelFirstd, ScaleIntensityd,\n",
        "                              RandSpatialCropd, RandAxisFlipd, RandGaussianNoised, RandGibbsNoised)\n",
        "from monai.losses import DiceLoss\n",
        "from monai.data import Dataset, DataLoader\n",
        "from monai.data.utils import pad_list_data_collate\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP4vzF-gyQBq"
      },
      "source": [
        "#Extracting Data From Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "k34swcSayS31",
        "outputId": "c177d304-66d1-4030-ef96-8c6972c19341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TkFZmNicGOfIRa-tUL-gxKx7k937IDPK&export=download\n",
            "From (redirected): https://drive.google.com/uc?id=1TkFZmNicGOfIRa-tUL-gxKx7k937IDPK&export=download&confirm=t&uuid=4d1700e7-f15d-4ce2-b887-79c484039085\n",
            "To: /content/CC_359.zip\n",
            "100%|██████████| 202M/202M [00:07<00:00, 26.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CC_359.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Replace this with your actual shareable link\n",
        "shareable_link = 'https://drive.google.com/file/d/1TkFZmNicGOfIRa-tUL-gxKx7k937IDPK/view?usp=sharing'\n",
        "\n",
        "# Extract file ID from the shareable link\n",
        "file_id = shareable_link.split('/d/')[1].split('/view')[0]\n",
        "\n",
        "# Create the direct download link\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}&export=download'\n",
        "\n",
        "# Specify the output file path\n",
        "output_file = 'CC_359.zip'\n",
        "\n",
        "# Download the file\n",
        "gdown.download(download_url, output_file, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqfkFuBJy71_",
        "outputId": "21bcf79d-d8b6-44cd-ffe2-2f42b60521a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  CC_359.zip\n",
            "   creating: CC_359/\n",
            "   creating: CC_359/imgs/\n",
            "  inflating: CC_359/imgs/CC0001_philips_15_55_M.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0001_philips_15_55_M_neg.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0002_philips_15_56_M.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0002_philips_15_56_M_neg.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0003_philips_15_63_F.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0003_philips_15_63_F_neg.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0004_philips_15_67_M.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0004_philips_15_67_M_neg.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0005_philips_15_62_M.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0005_philips_15_62_M_neg.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0006_philips_15_63_F.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0006_philips_15_63_F_neg.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0007_philips_15_62_M.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0007_philips_15_62_M_neg.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0008_philips_15_60_F.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0008_philips_15_60_F_neg.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0009_philips_15_69_M.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0009_philips_15_69_M_neg.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0010_philips_15_69_F.nii.gz  \n",
            "  inflating: CC_359/imgs/CC0010_philips_15_69_F_neg.nii.gz  \n",
            "   creating: CC_359/masks/\n",
            "  inflating: CC_359/masks/CC0001_philips_15_55_M_staple.nii.gz  \n",
            "  inflating: CC_359/masks/CC0001_philips_15_55_M_staple_neg.nii.gz  \n",
            "  inflating: CC_359/masks/CC0002_philips_15_56_M_staple.nii.gz  \n",
            "  inflating: CC_359/masks/CC0002_philips_15_56_M_staple_neg.nii.gz  \n",
            "  inflating: CC_359/masks/CC0003_philips_15_63_F_staple.nii.gz  \n",
            "  inflating: CC_359/masks/CC0003_philips_15_63_F_staple_neg.nii.gz  \n",
            "  inflating: CC_359/masks/CC0004_philips_15_67_M_staple.nii.gz  \n",
            "  inflating: CC_359/masks/CC0004_philips_15_67_M_staple_neg.nii.gz  \n",
            "  inflating: CC_359/masks/CC0005_philips_15_62_M_staple.nii.gz  \n",
            "  inflating: CC_359/masks/CC0005_philips_15_62_M_staple_neg.nii.gz  \n",
            "  inflating: CC_359/masks/CC0006_philips_15_63_F_staple.nii.gz  \n",
            "  inflating: CC_359/masks/CC0006_philips_15_63_F_staple_neg.nii.gz  \n",
            "  inflating: CC_359/masks/CC0007_philips_15_62_M_staple.nii.gz  \n",
            "  inflating: CC_359/masks/CC0007_philips_15_62_M_staple_neg.nii.gz  \n",
            "  inflating: CC_359/masks/CC0008_philips_15_60_F_staple.nii.gz  \n",
            "  inflating: CC_359/masks/CC0008_philips_15_60_F_staple_neg.nii.gz  \n",
            "  inflating: CC_359/masks/CC0009_philips_15_69_M_staple.nii.gz  \n",
            "  inflating: CC_359/masks/CC0009_philips_15_69_M_staple_neg.nii.gz  \n",
            "  inflating: CC_359/masks/CC0010_philips_15_69_F_staple.nii.gz  \n",
            "  inflating: CC_359/masks/CC0010_philips_15_69_F_staple_neg.nii.gz  \n",
            "  inflating: CC_359/imgs_list.xlsx   \n",
            "  inflating: CC_359/masks_list.xlsx  \n"
          ]
        }
      ],
      "source": [
        "!unzip CC_359.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iXDSmv9dP8yF"
      },
      "outputs": [],
      "source": [
        "# Images and masks directories\n",
        "img_dir = 'CC_359/imgs/'\n",
        "mask_dir = 'CC_359/masks/'\n",
        "\n",
        "# Loading excel files Containing the list of image and mask filenames:\n",
        "img_list = pd.read_excel('CC_359/imgs_list.xlsx')\n",
        "mask_list = pd.read_excel('CC_359/masks_list.xlsx')\n",
        "\n",
        "# Creating data dictionary\n",
        "data = []\n",
        "for img_name, mask_name in zip(img_list.iloc[:, 0], mask_list.iloc[:, 0]):\n",
        "    img_path = os.path.join(img_dir, img_name)\n",
        "    mask_path = os.path.join(mask_dir, mask_name)\n",
        "    if os.path.exists(img_path) and os.path.exists(mask_path):\n",
        "        data.append({\"img\": img_path, \"brain_mask\": mask_path, \"domain_label\": 0.0})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-heIpuNQhi6"
      },
      "source": [
        "##Building Dataset & Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zuo0S2ERELJC",
        "outputId": "5ae894f4-e57b-44f7-9717-7d8e5411d58c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed image shape: torch.Size([1, 112, 112, 112])\n"
          ]
        }
      ],
      "source": [
        "# Split data into training and testing sets (e.g., 80-20 split)\n",
        "train_size = int(0.8 * len(data))\n",
        "train_data, test_data = data[:train_size], data[train_size:]\n",
        "\n",
        "# Define the transformations using MONAI\n",
        "transform = Compose([\n",
        "    LoadImaged(keys=[\"img\", \"brain_mask\"]),\n",
        "    EnsureChannelFirstd(keys=[\"img\", \"brain_mask\"]),\n",
        "    ScaleIntensityd(keys=[\"img\"], minv=0.0, maxv=1.0),\n",
        "    RandSpatialCropd(keys=[\"img\",\"brain_mask\"], roi_size=(112, 112, 112), random_size=False),\n",
        "    RandAxisFlipd(keys=[\"img\", \"brain_mask\"], prob=0.2),\n",
        "    RandGaussianNoised(keys=[\"img\"], prob=0.2, mean=0.0, std=0.05),\n",
        "    RandGibbsNoised(keys=[\"img\"], prob=0.2, alpha=(0.1, 0.6))\n",
        "])\n",
        "\n",
        "train_dataset = monai.data.Dataset(data=train_data, transform=transform)\n",
        "test_dataset = monai.data.Dataset(data=test_data, transform=transform)\n",
        "print(\"Transformed image shape:\", train_dataset[0][\"img\"].shape)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building Data Loaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    collate_fn=pad_list_data_collate,\n",
        "    drop_last=False\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    collate_fn=pad_list_data_collate,\n",
        "    drop_last=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgQu4XwLtkRV"
      },
      "source": [
        "#Training Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sY1PtwHotnM7"
      },
      "outputs": [],
      "source": [
        "class TrainModel():\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.criterion = DiceLoss(to_onehot_y=False, sigmoid=True)\n",
        "\n",
        "    def dice_coefficient(self, outputs, masks, smooth=1e-5):\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        intersection = (outputs * masks).sum()\n",
        "        dice = (2. * intersection + smooth) / (outputs.sum() + masks.sum() + smooth)\n",
        "        return dice\n",
        "\n",
        "    def train_model(self, model, train_loader, learning_rate, epochs):\n",
        "        model.to(self.device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            correct_pixels = 0\n",
        "            total_pixels = 0\n",
        "            total_batches = len(train_loader)\n",
        "\n",
        "            for step, batch in enumerate(train_loader):\n",
        "                img = batch[\"img\"].to(self.device)\n",
        "                brain_mask = batch[\"brain_mask\"].to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                pred_mask = model(img)\n",
        "                loss = self.criterion(pred_mask, brain_mask)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                # Calculating accuracy (Pixel-level accuracy)\n",
        "                pred_mask = torch.sigmoid(pred_mask) > 0.5  # Converting to binary mask\n",
        "                correct_pixels += (pred_mask == brain_mask).sum().item()\n",
        "                total_pixels += brain_mask.numel()\n",
        "\n",
        "            epoch_loss = running_loss / total_batches\n",
        "            epoch_accuracy = correct_pixels / total_pixels\n",
        "\n",
        "            print(f\"Epoch {epoch}/{epochs}, Train Loss: {epoch_loss:.4f}, Accuracy: {(epoch_accuracy * 100):.2f}%\")\n",
        "\n",
        "        # Save the final model after training\n",
        "        torch.save(model.state_dict(), \"unet3d_model.pth\")\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apnzEELktoPF"
      },
      "source": [
        "#Evaluating Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAtRibeftp7d"
      },
      "outputs": [],
      "source": [
        "class EvalModel():\n",
        "    def __init__(self, model):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "    def test_model(self, test_loader):\n",
        "        self.model.eval()\n",
        "        dice_score_total = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, masks in test_loader:\n",
        "                inputs = inputs.to(self.device)\n",
        "                masks = masks.to(self.device)\n",
        "                outputs = self.model(inputs)\n",
        "\n",
        "                # Dice Score Calculation\n",
        "                dice_score = self.dice_coefficient(outputs, masks)\n",
        "                dice_score_total += dice_score\n",
        "\n",
        "        average_dice_score = dice_score_total / len(test_loader)\n",
        "        print(\"/------------------------------/\")\n",
        "        print(f\"Test Dice Score: {average_dice_score:.4f}\")\n",
        "        print(\"/------------------------------/\")\n",
        "        return average_dice_score\n",
        "\n",
        "    def dice_coefficient(self, outputs, masks):\n",
        "        smooth = 1e-6\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        outputs = (outputs > 0.5).float()\n",
        "        intersection = (outputs * masks).sum(dim=(2, 3))\n",
        "        union = outputs.sum(dim=(2, 3)) + masks.sum(dim=(2, 3))\n",
        "        dice = (2. * intersection + smooth) / (union + smooth)\n",
        "        return dice.mean().item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76gu36PtyOSK"
      },
      "source": [
        "#UNet3D Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "n-ixQbzb-XQO"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_block(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.convb = ConvBlock(in_channels, out_channels)\n",
        "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sc = self.convb(x) # Skip Connection\n",
        "        p = self.pool(sc)\n",
        "        return sc, p\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.convt = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.convb = ConvBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.convt(x1)\n",
        "        x1 = torch.cat([x1, x2], dim=1) # Concat with Skip Connection\n",
        "        x1 = self.convb(x1)\n",
        "        return x1\n",
        "\n",
        "class UNet3DModel(nn.Module):\n",
        "    def __init__(self, input_size=1, output_size=1):\n",
        "        super(UNet3DModel, self).__init__()\n",
        "        # Encoder\n",
        "        self.enc1 = EncoderBlock(input_size, 64)\n",
        "        self.enc2 = EncoderBlock(64, 128)\n",
        "        self.enc3 = EncoderBlock(128, 256)\n",
        "        self.enc4 = EncoderBlock(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bn = ConvBlock(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec1 = DecoderBlock(1024, 512)\n",
        "        self.dec2 = DecoderBlock(512, 256)\n",
        "        self.dec3 = DecoderBlock(256, 128)\n",
        "        self.dec4 = DecoderBlock(128, 64)\n",
        "\n",
        "        # Output Layer\n",
        "        self.out_conv = nn.Conv3d(64, output_size, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        sc1, p1 = self.enc1(x)\n",
        "        sc2, p2 = self.enc2(p1)\n",
        "        sc3, p3 = self.enc3(p2)\n",
        "        sc4, p4 = self.enc4(p3)\n",
        "\n",
        "        # Bottleneck\n",
        "        bn = self.bn(p4)\n",
        "\n",
        "        # Decoder\n",
        "        d1 = self.dec1(bn, sc4)\n",
        "        d2 = self.dec2(d1, sc3)\n",
        "        d3 = self.dec3(d2, sc2)\n",
        "        d4 = self.dec4(d3, sc1)\n",
        "\n",
        "        # Output Layer\n",
        "        out = self.out_conv(d4)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIURp6o2rTqg"
      },
      "source": [
        "##Model Summery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1X2ZfhRrWPT"
      },
      "outputs": [],
      "source": [
        "input_size = (1, 128, 128, 128)\n",
        "unet3d_model = UNet3DModel().to(device)\n",
        "summary(unet3d_model, input_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mef5bPrVv34s"
      },
      "source": [
        "##Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "-Pap_0F9v6se",
        "outputId": "1038ba58-11ce-438b-bb4b-7515313f7ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Train Loss: 0.3431, Accuracy: 77.72%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-bfeaca3a5f84>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m trained_unet3d_model = trainer.train_model(unet3d_model, train_dataloader,\n\u001b[0m\u001b[1;32m      8\u001b[0m                                            learning_rate=LEARNING_RATE, epochs=EPOCHS)\n\u001b[1;32m      9\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_unet3d_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unet3d_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-7c87cd3b4d5d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model, train_loader, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtotal_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mbrain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"brain_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# dataset[[1, 3, 4]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m     93\u001b[0m         \u001b[0mdata_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mslice\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/transforms/compose.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0m_lazy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlazy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         result = execute_compose(\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/transforms/compose.py\u001b[0m in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0m_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mThreadUnsafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         data = apply_transform(\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmap_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_apply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_apply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# if in debug mode, don't swallow exception so that the breakpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyTrait\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyTrait\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/transforms/io/dictionary.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, reader)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_key_postfix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_key_postfix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/transforms/io/array.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, filename, reader)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mimg_array\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNdarrayOrTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_dst_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/data/image_reader.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMetaKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPATIAL_SHAPE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_spatial_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMetaKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPACE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpaceKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRAS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_array_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_non_spatial_dims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMetaKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPATIAL_SHAPE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/data/image_reader.py\u001b[0m in \u001b[0;36m_get_array_data\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \"\"\"\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "unet3d_model = UNet3DModel().to(device)\n",
        "\n",
        "# Training Hyperparameters\n",
        "EPOCHS = 500\n",
        "LEARNING_RATE = 0.001\n",
        "trainer = TrainModel()\n",
        "trained_unet3d_model = trainer.train_model(unet3d_model, train_dataloader,\n",
        "                                           learning_rate=LEARNING_RATE, epochs=EPOCHS)\n",
        "torch.save(trained_unet3d_model.state_dict(), 'unet3d_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b94_glsmXu5e",
        "outputId": "f5196ff2-4777-49bb-c932-8501de9f9350"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.0+cu121\n",
            "CUDA version: 12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8hcHGl_v7mc"
      },
      "source": [
        "##Evaluating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QPUscYev_d8"
      },
      "outputs": [],
      "source": [
        "eval_model = EvalModel(trained_unet3d_model)\n",
        "test_accuracy = eval_model.test_model(test_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_JlrYL-Th72w",
        "MoOkszKBB7nv",
        "Im4aucsWdm-E"
      ],
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCv4UKFh0toNSFQYuEjheF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}